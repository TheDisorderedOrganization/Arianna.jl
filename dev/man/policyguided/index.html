<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Policy-guided Monte Carlo · Arianna</title><meta name="title" content="Policy-guided Monte Carlo · Arianna"/><meta property="og:title" content="Policy-guided Monte Carlo · Arianna"/><meta property="twitter:title" content="Policy-guided Monte Carlo · Arianna"/><meta name="description" content="Documentation for Arianna."/><meta property="og:description" content="Documentation for Arianna."/><meta property="twitter:description" content="Documentation for Arianna."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="Arianna logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">Manual</span><ul><li><a class="tocitem" href="../montecarlo/">Markov Chain Monte Carlo</a></li><li><a class="tocitem" href="../system/">Adding Your Own System</a></li><li class="is-active"><a class="tocitem" href>Policy-guided Monte Carlo</a><ul class="internal"><li><a class="tocitem" href="#Implementation-in-Arianna"><span>Implementation in Arianna</span></a></li><li><a class="tocitem" href="#Running-a-PGMC-simulation"><span>Running a PGMC simulation</span></a></li></ul></li></ul></li><li><a class="tocitem" href="../../related/">Related packages</a></li><li><a class="tocitem" href="../../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Manual</a></li><li class="is-active"><a href>Policy-guided Monte Carlo</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Policy-guided Monte Carlo</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/TheDisorderedOrganization/Arianna.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/TheDisorderedOrganization/Arianna.jl/blob/main/docs/src/man/policyguided.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Policy-guided-Monte-Carlo"><a class="docs-heading-anchor" href="#Policy-guided-Monte-Carlo">Policy-guided Monte Carlo</a><a id="Policy-guided-Monte-Carlo-1"></a><a class="docs-heading-anchor-permalink" href="#Policy-guided-Monte-Carlo" title="Permalink"></a></h1><p>Policy-guided Monte Carlo (PGMC) is an <strong>adaptive Monte Carlo method</strong> that dynamically adjusts the proposal distribution in the Metropolis-Hastings (MH) kernel to <strong>maximise sampling efficiency</strong>, using a formalism inspired by <strong>reinforcement learning</strong>.</p><p>As long as the proposal distribution <span>$Q$</span> guarantees ergodicity, here is significant flexibility in the choice of its specific form. PGMC aims at finding an optimal proposal distribution that maximises some measure of  efficiency of the Markov chain. To do this, it needs a <strong>reward function</strong> <span>$r\left(x,x&#39;\right)$</span> that quantifies the performance of a single transition <span>$x\to x&#39;$</span>. The reward function must satisfy the constraint <span>$r\left(x,x&#39;\right)=0$</span>. This can be used to define the <strong>objective function</strong></p><p class="math-container">\[J\left(Q\right)=\mathbb E_{\substack{x\sim P \\ x&#39;\sim K}}\left[r\left(x,x&#39;\right)\right],\]</p><p>that is nothing but the average reward over the Markov chain.</p><p>The goal is to find a proposal distribution <span>$Q^\star$</span> that maximises the objective function <span>$J$</span>. To practically tackle the problem, we restrict the search to a family of distributions <span>$Q_{\theta}$</span>​ parameterised by a real vector <span>$\theta$</span>. Starting from an initial guess, we then update <span>$\theta$</span> iteratively according to the <strong>stochastic gradient ascent</strong> procedure</p><p class="math-container">\[\theta\leftarrow\theta +\eta\,\widehat{\nabla_\theta J},\]</p><p>where <span>$\eta$</span> is the learning rate and <span>$\widehat{\nabla_\theta J}$</span> is a stochastic estimate of the actual gradient of <span>$J$</span> with respect to <span>$\theta$</span>.</p><h2 id="Implementation-in-Arianna"><a class="docs-heading-anchor" href="#Implementation-in-Arianna">Implementation in Arianna</a><a id="Implementation-in-Arianna-1"></a><a class="docs-heading-anchor-permalink" href="#Implementation-in-Arianna" title="Permalink"></a></h2><p>Arianna implements PGMC through two core algorithms found in the submodule <code>PolicyGuided</code>:</p><ul><li><code>PolicyGradientEstimator</code> computes <span>$\widehat{\nabla_\theta J}$</span> for each move in the pool by drawing multiple samples from <span>$P$</span> and <span>$Q$</span>.</li><li><code>PolicyGradientUpdate</code> applies the estimated gradient to update the move parameters. The <code>PolicyGuided</code> module provides several advanced optimisers, including natural gradients methods.</li></ul><h2 id="Running-a-PGMC-simulation"><a class="docs-heading-anchor" href="#Running-a-PGMC-simulation">Running a PGMC simulation</a><a id="Running-a-PGMC-simulation-1"></a><a class="docs-heading-anchor-permalink" href="#Running-a-PGMC-simulation" title="Permalink"></a></h2><p>To make your Monte Carlo simulation adaptive, simply add the two algorithms from PolicyGuided to the simulation. The following Julia script demonstrates this in the <a href="https://github.com/TheDisorderedOrganization/Arianna.jl/blob/main/example/particle_1d/particle_1d.jl">particle_1D.jl</a> example</p><pre><code class="language-julia hljs">include(&quot;example/particle_1D/particle_1d.jl&quot;)

x₀ = 0.0
β = 2.0
M = 10
chains = [System(x₀, β) for _ in 1:M]
pool = (Move(Displacement(0.0), StandardGaussian(), ComponentArray(σ=0.1), 1.0),)
seed = 42
optimisers = (VPG(0.001),)
steps = 10^5
burn = 1000
sampletimes = build_schedule(steps, burn, 10)
path = &quot;data/PGMC/particle_1d/Harmonic/beta$β/M$M/seed$seed&quot;
algorithm_list = (
    (algorithm=Metropolis, pool=pool, seed=seed, parallel=false),
    (algorithm=PolicyGradientEstimator, dependencies=(Metropolis,), optimisers=optimisers, parallel=false),
    (algorithm=PolicyGradientUpdate, dependencies=(PolicyGradientEstimator,), scheduler=build_schedule(steps, burn, 2)),
    (algorithm=StoreCallbacks, callbacks=(callback_energy, callback_acceptance), scheduler=sampletimes),
    (algorithm=StoreTrajectories, scheduler=sampletimes),
    (algorithm=StoreParameters, dependencies=(Metropolis,), scheduler=sampletimes),
    (algorithm=PrintTimeSteps, scheduler=build_schedule(steps, burn, steps ÷ 10)),
)
simulation = Simulation(chains, algorithm_list, steps; path=path, verbose=true)
run!(simulation)</code></pre><p>In this example, PGMC optimises the standard deviation <code>σ</code> of the Gaussian-distributed displacements using the <code>VPG</code> optimiser with a learing rate of <code>0.001</code>. Note that <code>PolicyGradientUpdate</code> is called every two calls of <code>PolicyGradientEstimator</code> to accumulate more samples for gradient estimation before each update.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../system/">« Adding Your Own System</a><a class="docs-footer-nextpage" href="../../related/">Related packages »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.1 on <span class="colophon-date" title="Tuesday 4 March 2025 14:53">Tuesday 4 March 2025</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
